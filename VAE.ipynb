{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dim, latent_dim,\n",
    "        encoder_sizes=(64, 64), \n",
    "        decoder_sizes=(64, 64),\n",
    "        beta=1.0, L=1\n",
    "    ):\n",
    "        \"\"\" Basic Variational Autoencoder with Standard Normal prior\n",
    "            :param data_dim: original data dimensionality        \n",
    "            :param latent_dim: latent space dimensionality\n",
    "            :param encoder_sizes: hidden layer sizes for the encoder network\n",
    "            :param decoder_sizes: hidden layer sizes for the decoder network\n",
    "            :param beta: tradeoff coefficient between reconstruction and KL terms in ELBO\n",
    "            :param L: number of Monte Carlo samples for ELBO estimation\n",
    "        \"\"\"        \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dim = data_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.L = L\n",
    "        \n",
    "        self.encoder = self.build_encoder(data_dim, encoder_sizes, latent_dim)\n",
    "        \n",
    "    def build_encoder(self, data_dim, encoder_sizes, latent_dim):        \n",
    "        modules = []\n",
    "        for i in len(encoder_sizes):\n",
    "            if i==0:\n",
    "                modules.append(nn.Linear(data_dim, encoder_sizes[i]))\n",
    "            else:\n",
    "                modules.append(nn.Linear(encoder_sizes[i-1], encoder_sizes[i]))\n",
    "        modules.append(nn.Linear(encoder_sizes[-1], 2*latent_dim))\n",
    "\n",
    "        return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "  (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (2): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
